# 目录

[TOC]

# 第一章	网络爬虫入门

## 第一课	网络爬虫概述

问题1：为啥使用python进行爬虫？

答：比较方便，简洁，第三方库的支持

问题2：网络爬虫是否合法？

答：爬虫是允许的，但要在法律的范围之内去使用，不能反复攻击网站....

问题3：反爬机制与反反爬机制的区别

反爬机制指的是网站为了避免爬虫程序的爬取所设置的一个安全措施

反反爬机制指的是爬虫程序为了爬取数据所进行绕过网络避免爬虫程序爬取的安全的一种措施

问题4：robots.txt协议是什么？

答：君子之间的协定（即爬虫所操纵的用户与服务器的提供商之间的约定），规避了网站的哪些网页是否可以被爬取。

## 第二课	WEB请求解析

（1）服务器渲染：将web请求到的数据在服务器那边将数据进行整合

![image-20230127152738754](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230127152738754.png)

（2）客户端渲染：将web请求到的数据在客户端这里进行整合（第一次返回的是http框架，第二次是网页所包含的信息），爬取这种网页只需要关注带信息的请求地址即可

![image-20230127153042312](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230127153042312.png)

## 第三课	简单的爬虫程序

（1）urllib库：python中内置的爬虫库，可以爬取网站的内容

案例：使用urllib库爬取gitee网站

```python
#使用urllib库爬取gitee网站首页：https://www.gitee.com/
from urllib.request import * #导入urlopen模块
url="http://www.gitee.com/"#设置url
resp=urlopen(url=url) #使用urlopen打开网址
with open("gitee网站.html",mode='w+',encoding='utf-8') as fp:
    fp.write(resp.read().decode("utf-8"))
fp.close()
resp.close()
print("OK")
```

------

解析：

1、urlopen的使用

urlopen(url(网址),data（传递的数据）)

2、open函数

```python
def open(file, mode='r', buffering=None, encoding=None, errors=None, newline=None, closefd=True)
'''
file	代表打开文件位置
mode	代表对文件进行什么操作
buffering	代表文件的缓存
encoding	代表对文件进行操作的编码格式
其他略
'''
```

3、decode(解码)与encode(编码)的区别

| decode                                                       |                            encode                            |
| ------------------------------------------------------------ | :----------------------------------------------------------: |
| decode的作用是将其他编码的字符串转换成unicode编码，如str1.decode(‘gb2312’)，表示将gbk编码的字符串str1转换成unicode编码。 | encode的作用是将unicode编码转换成其他编码的字符串，如str2.encode(‘gb2312’)，表示将unicode编码的字符串str2转换成gbk编码。 |

常见中文编码：GBK，GB2312,UTF-8

`爬虫的文件出现中文乱码的时候可以检查一下open函数中的encoding是否设置于爬取的网页相同的编码`

（2）requests库:第三方库，比urllib库更加强大，实现的功能会更多

（1）问题1：如何安装requests

​			答：安装：pip install requests

（2）案例：

1、获取gitee首页源码

```python
#使用requests库爬取gitee网站首页：https://www.gitee.com/
import requests
url="https://www.gitee.com/"
resp=requests.get(url=url)
resp.encoding='utf-8'#设置获取到中文的编码为utf-8
with open("gitee网站2.html",mode='w+',encoding='utf-8') as fp:
    fp.write(resp.text)
fp.close()
resp.close()
```

2、使用百度查询信息

```python
import requests
header={
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36 Edg/109.0.1518.69"
}
word=input("请输入查询信息")
url="https://www.sogou.com/web?query={}".format(word)
resp=requests.get(url=url,headers=header)
resp.encoding='utf-8'

with open("./pages/{}.html".format(word),mode='w+',encoding='utf-8') as fp:
    fp.write(resp.text)
    fp.close()
resp.close()
print("OK")
```

------

解析：

2-1、request.get	使用get请求访问网址

get可携带的参数有url,data,其他参数（可以带参数，但是**关键字是params不是data**）

2-2、requests设置统一的编码格式：resp.encoding="编码"

2-3、显示爬取到的网页源代码使用resp.text,显示爬取到图片，视频等内容使用resp.contant

注：如果request.text爬出的结果是空的，那么就需要加上一个ua进行反反爬虫

3、使用百度翻译查询翻译结果

```python
import requests
keywords=input("请输入要查询的内容")
con={
 "kw":"{}".format(keywords)
}
url="https://fanyi.baidu.com/sug"
req=requests.post(url=url,data=con)
# print(req.json()['data'][0]['v'])
for i in range(0,len(req.json()['data'])):
 print(req.json()['data'][i]['k']+" "* 5+req.json()['data'][i]['v'])
```

解析：

3-1、request.post 使用post请求访问网址

post可携带的参数有url,data及其他参数（**data是传参时必须携带的**）

3-2 request.json() 将展示结果使用json字符串表示出来

4、爬取豆瓣喜剧排行榜

```python
import requests
url='https://movie.douban.com/j/chart/top_list'
headers={
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36 Edg/109.0.1518.69"
}
for first in range(0,101,10):
    for i in range(0,1000,20):
        '''
        datas={
            "type": "24",
            "interval_id": "{}:{}".format(first+10,first),
            "action":"",
            "start": "{}".format(i),
            "limit": "20"
        }
        将构建的参数传入发送请求的get中
        '''
        datas={
            "type": "24",
            "interval_id": "{}:{}".format(first+10,first),
            "action":"",
            "start": "{}".format(i),
            "limit": "20"
        }
        if(first>100 or first+10==100):
            break
        resp=requests.get(url=url,params=datas,headers=headers)
        resp.encoding='utf-8'
        with open('./results/豆瓣电影喜剧评分{}-{}.json'.format(first+10,first),mode='a+',encoding='utf-8') as fp:
            fp.write(str(resp.json()))
        print("正在爬取豆瓣评分{}-{}喜剧电影,已经爬取{}个内容".format(first+10,first,i))
fp.close()
resp.close()
print(resp.request.headers)
print("OK")
```

解析

4-1、resp.request.url 获取通过拼接后的url

4-2、resp.request.head 获取当前的头信息

4-3、request中get格式：requests.get(url=url,params=datas,headers=headers)

# 第二章	数据解析与抓取

## 第一课	数据解析概论

问：什么是数据解析

答：第一章节我们学习了从页面中提取数据的源代码，但是有的时候我们想要的数据而不是页面源代码，那么怎么做？并且我们所提取的信息都是夹在html语言中，那么怎么提取？那么就需要用到我们的数据解析，数据解析是解析所要抓取的页面源代码中提取某些重要的信息，比如：标题，内容等

问：使用什么工具对页面进行解析

答：re解析，bs4解析，xpath解析

## 第二课 正则表达式*

（1）概述：使用一种表达式的方法来表示字符串的内容

（2）语法：使用**元字符**进行排列组合来匹配字符串

元字符：正则表达式中的表达式所携带特殊意义的符号，**默认只匹配一个字符**

常见的元字符如下：

附件：

[开源中国在线正则表达式测试]: https://tool.oschina.net/regex	"在线正则表达式测试"

| 正则表达式符号 |        正则表达式符号含义        |
| :------------: | :------------------------------: |
|       .        |    匹配除换行符之外的任意字符    |
|       \w       | 匹配字母数值，下划线[0-9A-Za-z_] |
|       \s       |         匹配任意空白字符         |
|       \d       |             匹配数字             |
|    \n 、\t     |         换行、水平制表符         |
|       ^        |       匹配字符串的开始/非        |
|       $        |         匹配字符串的结束         |
|     **\W**     |    **非匹配字母数值，下划线**    |
|     **\D**     |          **匹配非数值**          |
|     **\S**     |        **匹配非空白字符**        |
|      a\|b      |           匹配a或b字符           |
|      （）      |      预先匹配括号内的表达式      |
|       []       |        匹配字符组内的内容        |
|      [^]       |        匹配字符组外的内容        |

量词

| 正则表达式符号 |  正则表达式符号含义  |
| :------------: | :------------------: |
|       *        | 满足表达式零次或多次 |
|       +        | 满足表达式一次或多次 |
|       ？       | 满足条件或不满足条件 |
|      {n}       |       重复n次        |
|      {n,}      |    重复n次或更多     |
|     {n,m}      |      重复n到m次      |

贪婪匹配与惰性匹配

| 正则表达式符号 |  正则表达式符号含义  |           备注           |
| :------------: | :------------------: | :----------------------: |
|       .*       |       贪婪匹配       | 尽可能多的去满足条件需求 |
|      .*?       |       惰性匹配       | 尽可能少的去满足条件需求 |
|    .*?内容     | 内容重复找最小的一次 |                          |

注：

（1）例子：

符号：*

匹配文本：我的电话是：88023021

正则表达式：\d*

匹配结果：（空格） 数字 （空格）

解析过程：‘我的电话是：’是不是满足正则，不满足，那算0次，那么就分别用换行分别替代，后面的数字满足条件，输出

结论：不管满不满足数值我都显示出来，只不过满足显示内容，不满足用换行代替

（2）例子：

符号：.\*?与.*

匹配文本：*玩吃鸡游戏，晚上打游戏，打什么游戏，吃鸡游戏啊，你不会打吧？*

正则表达式：玩.\*?游戏

匹配结果：玩吃鸡游戏

解析过程：

1、文本：玩吃鸡游戏，晚上打游戏，打什么游戏，吃鸡游戏啊，你不会打这个游戏吧？

2、表达式：玩.\*?游戏

3、搜索过程：只要满足玩	中间是什么随便	结尾是游戏的格式即可

第一遍：玩吃鸡游戏

第二遍：玩吃鸡游戏，晚上打游戏

第三遍：玩吃鸡游戏，晚上打游戏，打什么游戏

第四遍：玩吃鸡游戏，晚上打游戏，打什么游戏，吃鸡游戏

第五遍：玩吃鸡游戏，晚上打游戏，打什么游戏，吃鸡游戏啊，你不会打这个游戏

稍微修改一下内容结果就大不相同

1、文本：玩吃鸡游戏，晚上打游戏，**玩**什么游戏，吃鸡游戏啊，你不会打这个游戏吧？

2、表达式：玩.\*?游戏

3、搜索过程：只要满足玩	中间是什么随便	结尾是游戏的格式即可

第一遍：玩吃鸡游戏

第二遍：玩吃鸡游戏，晚上打游戏

**第三遍：玩什么游戏**

第四遍：玩什么游戏，吃鸡游戏

第五遍：玩什么游戏，吃鸡游戏啊，你不会打这个游戏

结果：

1、玩吃鸡游戏

2、玩什么游戏

由于.*?是取在此字符串中最少的一个内容，那么他里面的内容就是最后的内容，实际上他第一次找最长的由于？的限制，他只能往后找第二..nc长的一个内容，采用回溯的方式。

（3）例子

1、文本：<div class='jay'>周杰伦</div><div class='ljj'>林俊杰</div>

2、表达式：<div class=.*?>[.*?]</div>

3、搜索过程：只要满足玩<div class= xxx（内容显示最少）>内容显示最少</div>即可

第一遍：<div class='jay'>周杰伦</div><div class='ljj'>林俊杰</div>

第二遍：<div class='jay'>周杰伦</div>

第三遍：<div class='ljj'>林俊杰</div>

4、结果：

\<div class='jay'>周杰伦\</div>

\<div class='ljj'>林俊杰\</div>

（4）例子

1、文本：abhjkjbaskasdkzbjhasbdjkbakdskjz

2、表达式：.*z

3、搜索过程：只要满足结果是z结尾的即可，开头不限制

第一遍：abhjkjbaskasdkzbjhasbdjkbakdskjz

第二遍：abhjkjbaskasdkz

第三遍：bjhasbdjkbakdskjz

结果：

abhjkjbaskasdkz

bjhasbdjkbakdskjz

## 第三课	re模块

