# 第一章	网络爬虫入门

## 第一课	网络爬虫概述

问题1：为啥使用python进行爬虫？

答：比较方便，简洁，第三方库的支持

问题2：网络爬虫是否合法？

答：爬虫是允许的，但要在法律的范围之内去使用，不能反复攻击网站....

问题3：反爬机制与反反爬机制的区别

反爬机制指的是网站为了避免爬虫程序的爬取所设置的一个安全措施

反反爬机制指的是爬虫程序为了爬取数据所进行绕过网络避免爬虫程序爬取的安全的一种措施

问题4：robots.txt协议是什么？

答：君子之间的协定（即爬虫所操纵的用户与服务器的提供商之间的约定），规避了网站的哪些网页是否可以被爬取。

## 第二课	WEB请求解析

（1）服务器渲染：将web请求到的数据在服务器那边将数据进行整合

![image-20230127152738754](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230127152738754.png)

（2）客户端渲染：将web请求到的数据在客户端这里进行整合（第一次返回的是http框架，第二次是网页所包含的信息），爬取这种网页只需要关注带信息的请求地址即可

![image-20230127153042312](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230127153042312.png)

## 第三课	简单的爬虫程序

（1）urllib库：python中内置的爬虫库，可以爬取网站的内容

案例：使用urllib库爬取gitee网站

```python
#使用urllib库爬取gitee网站首页：https://www.gitee.com/
from urllib.request import * #导入urlopen模块
url="http://www.gitee.com/"#设置url
resp=urlopen(url=url) #使用urlopen打开网址
with open("gitee网站.html",mode='w+',encoding='utf-8') as fp:
    fp.write(resp.read().decode("utf-8"))
fp.close()
resp.close()
print("OK")
```

------

解析：

1、urlopen的使用

urlopen(url(网址),data（传递的数据）)

2、open函数

```python
def open(file, mode='r', buffering=None, encoding=None, errors=None, newline=None, closefd=True)
'''
file	代表打开文件位置
mode	代表对文件进行什么操作
buffering	代表文件的缓存
encoding	代表对文件进行操作的编码格式
其他略
'''
```

3、decode(解码)与encode(编码)的区别

| decode                                                       |                            encode                            |
| ------------------------------------------------------------ | :----------------------------------------------------------: |
| decode的作用是将其他编码的字符串转换成unicode编码，如str1.decode(‘gb2312’)，表示将gbk编码的字符串str1转换成unicode编码。 | encode的作用是将unicode编码转换成其他编码的字符串，如str2.encode(‘gb2312’)，表示将unicode编码的字符串str2转换成gbk编码。 |

常见中文编码：GBK，GB2312,UTF-8

`爬虫的文件出现中文乱码的时候可以检查一下open函数中的encoding是否设置于爬取的网页相同的编码`

（2）requests库:第三方库，比urllib库更加强大，实现的功能会更多

（1）问题1：如何安装requests

​			答：安装：pip install requests

（2）案例：

1、获取gitee首页源码

```python
#使用requests库爬取gitee网站首页：https://www.gitee.com/
import requests
url="https://www.gitee.com/"
resp=requests.get(url=url)
resp.encoding='utf-8'#设置获取到中文的编码为utf-8
with open("gitee网站2.html",mode='w+',encoding='utf-8') as fp:
    fp.write(resp.text)
fp.close()
resp.close()
```

2、使用百度查询信息

```python
import requests
header={
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36 Edg/109.0.1518.69"
}
word=input("请输入查询信息")
url="https://www.sogou.com/web?query={}".format(word)
resp=requests.get(url=url,headers=header)
resp.encoding='utf-8'

with open("./pages/{}.html".format(word),mode='w+',encoding='utf-8') as fp:
    fp.write(resp.text)
    fp.close()
resp.close()
print("OK")
```

------

解析：

2-1、request.get	使用get请求访问网址

get可携带的参数有url,data,其他参数（可以带参数，但是**关键字是params不是data**）

2-2、requests设置统一的编码格式：resp.encoding="编码"

2-3、显示爬取到的网页源代码使用resp.text,显示爬取到图片，视频等内容使用resp.contant

注：如果request.text爬出的结果是空的，那么就需要加上一个ua进行反反爬虫

3、使用百度翻译查询翻译结果

```python
import requests
keywords=input("请输入要查询的内容")
con={
 "kw":"{}".format(keywords)
}
url="https://fanyi.baidu.com/sug"
req=requests.post(url=url,data=con)
# print(req.json()['data'][0]['v'])
for i in range(0,len(req.json()['data'])):
 print(req.json()['data'][i]['k']+" "* 5+req.json()['data'][i]['v'])
```

解析：

3-1、request.post 使用post请求访问网址

post可携带的参数有url,data及其他参数（**data是传参时必须携带的**）

3-2 request.json() 将展示结果使用json字符串表示出来

4、爬取豆瓣喜剧排行榜



解析

4-1、resp.request.url 获取通过拼接后的url

4-2、resp.request.head 获取当前的头信息



