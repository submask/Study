# 第一章	网络爬虫入门

## 第一课	网络爬虫概述

问题1：为啥使用python进行爬虫？

答：比较方便，简洁，第三方库的支持

问题2：网络爬虫是否合法？

答：爬虫是允许的，但要在法律的范围之内去使用，不能反复攻击网站....

问题3：反爬机制与反反爬机制的区别

反爬机制指的是网站为了避免爬虫程序的爬取所设置的一个安全措施

反反爬机制指的是爬虫程序为了爬取数据所进行绕过网络避免爬虫程序爬取的安全的一种措施

问题4：robots.txt协议是什么？

答：君子之间的协定（即爬虫所操纵的用户与服务器的提供商之间的约定），规避了网站的哪些网页是否可以被爬取。

## 第二课	WEB请求解析

（1）服务器渲染：将web请求到的数据在服务器那边将数据进行整合

![image-20230127152738754](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230127152738754.png)

（2）客户端渲染：将web请求到的数据在客户端这里进行整合（第一次返回的是http框架，第二次是网页所包含的信息），爬取这种网页只需要关注带信息的请求地址即可

![image-20230127153042312](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230127153042312.png)

## 第三课	简单的爬虫程序

（1）urllib库：python中内置的爬虫库，可以爬取网站的内容

案例：使用urllib库爬取gitee网站

```python
#使用urllib库爬取gitee网站首页：https://www.gitee.com/
from urllib.request import * #导入urlopen模块
url="http://www.gitee.com/"#设置url
resp=urlopen(url=url) #使用urlopen打开网址
with open("gitee网站.html",mode='w+',encoding='utf-8') as fp:
    fp.write(resp.read().decode("utf-8"))
fp.close()
resp.close()
print("OK")
```

解析：

（1）urlopen的使用

urlopen(url(网址),data（传递的数据）)

（2）open函数

```python
def open(file, mode='r', buffering=None, encoding=None, errors=None, newline=None, closefd=True)
'''
file	代表打开文件位置
mode	代表对文件进行什么操作
buffering	代表文件的缓存
encoding	代表对文件进行操作的编码格式
其他略
'''
```

（3）decode(解码)与encode(编码)的区别

| decode                                                       |                            encode                            |
| ------------------------------------------------------------ | :----------------------------------------------------------: |
| decode的作用是将其他编码的字符串转换成unicode编码，如str1.decode(‘gb2312’)，表示将gbk编码的字符串str1转换成unicode编码。 | encode的作用是将unicode编码转换成其他编码的字符串，如str2.encode(‘gb2312’)，表示将unicode编码的字符串str2转换成gbk编码。 |

常见中文编码：GBK，GB2312,UTF-8

`爬虫的文件出现中文乱码的时候可以检查一下open函数中的encoding是否设置于爬取的网页相同的编码`

（4）requests库:第三方库，比urllib库更加强大，实现的功能会更多

问题1：如何安装requests

答：安装：pip install requests

